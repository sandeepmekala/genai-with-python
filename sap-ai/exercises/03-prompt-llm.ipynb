{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your connection to Generative AI Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è In the init_env.py file the values from `generative-ai-codejam/.aicore-config.json` are assigned to environmental variables. That way the Generative AI Hub [Python SDK](https://pypi.org/project/generative-ai-hub-sdk/) will connect to Generative AI Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ For the Python SDK to know which resource group to use, you also need to set the `resource group` in the [`variables.py`](variables.py) file to your own `resource group` (e.g. **team-01**) that you created in the SAP AI Launchpad in exercise [00-connect-AICore-and-AILaunchpad](00-connect-AICore-and-AILaunchpad.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource group is set to: default\n"
     ]
    }
   ],
   "source": [
    "import init_env\n",
    "import variables\n",
    "import importlib\n",
    "variables = importlib.reload(variables)\n",
    "\n",
    "# TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# in genAI Hub. You might also want to chose a model that can also process images here already. \n",
    "# E.g. 'gpt-4.1-mini'\n",
    "MODEL_NAME = 'gpt-4.1-mini'\n",
    "# Do not modify the `assert` line below\n",
    "assert MODEL_NAME!='', \"\"\"You should change the variable `MODEL_NAME` with the name of your deployed model (like 'gpt-4o-mini') first!\"\"\"\n",
    "\n",
    "init_env.set_environment_variables()\n",
    "# Do not modify the `assert` line below \n",
    "assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt an LLM in the Generative AI Hub...\n",
    "\n",
    "...using OpenAI native client integration: https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/gen_ai_hub.html#openai\n",
    "\n",
    "To understand the API and structures, check OpenAI documentation: https://platform.openai.com/docs/guides/text?api-mode=chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "An LLM is typically based on the Transformer architecture, which uses layers of self-attention mechanisms and feed-forward neural networks to process and generate language by modeling token relationships in sequences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "from IPython.display import Markdown\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding roles\n",
    "Most LLMs have the roles `system`, `assistant` (GPT) or `model` (Gemini) and `user` that can be used to steer the models response. In the previous step you only used the role `user` to ask your question. \n",
    "\n",
    "üëâ Try out different `system` messages to change the response. You can also tell the model to not engage in smalltalk or only answer questions on a certain topic. Then try different user prompts as well!\n",
    "\n",
    "Please note, that in OpenAI API with `o1` models and newer, `developer` messages replace the previous `system` messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Arrr! The underlying model architecture o' an LLM be the Transformer, a mighty neural network that uses self-attention to weigh the importance o' each word in a sentence, makin' it savvy at understandin' and generatin' language, like a clever pirate readin' the winds! Yarrr!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like a Pirate.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak like a pirate.\n",
    "\n",
    "üëâ Now let's be more serious! Tell it to behave like an SAP consultant talking to AI Developers.\n",
    "\n",
    "üëâ Ask it to behave like an SAP Consultant talking to ABAP Developers and to make ABAP comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "üëâ Run the following question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Data masking in the context of an orchestration service‚Äîsuch as one used within SAP Integration Suite or SAP Cloud Platform Integration‚Äîtypically refers to the process of concealing sensitive data elements in message payloads during processing or monitoring, to ensure data privacy and compliance.\n",
       "\n",
       "Here is how data masking generally works in an orchestration service:\n",
       "\n",
       "1. **Identification of Sensitive Fields:**  \n",
       "   The orchestration service is configured with rules or policies that specify which data fields should be masked. These are usually sensitive elements like personally identifiable information (PII), financial data, or credentials.\n",
       "\n",
       "2. **Masking Technique:**  \n",
       "   When a message passes through the orchestration, the service applies masking techniques such as:\n",
       "   - Replacing the actual values with a fixed mask like asterisks (e.g., *****)\n",
       "   - Partial masking, where part of the value remains visible (e.g., last 4 digits of a credit card)\n",
       "   - Substitution with generic or randomized characters\n",
       "\n",
       "3. **Masking Scope:**  \n",
       "   Masking can be applied at different stages:\n",
       "   - **Logging and Monitoring:** To prevent sensitive data from being stored in logs or visible in monitoring tools.\n",
       "   - **Message Content:** To ensure that downstream services or recipients only receive masked data if appropriate.\n",
       "   \n",
       "4. **Configuration:**  \n",
       "   In SAP Integration Suite (CPI), data masking can be set up via:\n",
       "   - Data store configurations\n",
       "   - Content modifier steps that replace values in the message payload\n",
       "   - Custom scripts (Groovy/JavaScript) that selectively mask data dynamically\n",
       "   - Data protection policies that automatically redact or obfuscate sensitive information during message processing or audit logging\n",
       "\n",
       "5. **Security and Compliance:**  \n",
       "   Data masking helps meet regulatory requirements like GDPR or HIPAA by preventing exposure of confidential data during integration or message handling.\n",
       "\n",
       "**Summary:**  \n",
       "Data masking in orchestration services hides sensitive data fields in the message content or logs by applying predefined rules or scripts, ensuring that sensitive information is not exposed during message processing, monitoring, or logging. This maintains security and compliance without impacting the overall message flow.\n",
       "\n",
       "If you‚Äôre working with a specific SAP orchestration service (e.g., SAP Integration Suite), I can provide detailed steps or examples on how to implement data masking in that environment. Let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        \"content\": \"You are an SAP Consultant.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How does the data masking of the orchestration service work?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Compare the response to [SAP Help - Generative AI Hub SDK](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html). \n",
    "\n",
    "üëâ What did the model respond? Was it the truth or a hallucination?\n",
    "\n",
    "üëâ Which questions work well, which questions still do not work so well?\n",
    "\n",
    "# Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and [images](https://platform.openai.com/docs/guides/images-vision?api-mode=chat&format=base64-encoded#giving-a-model-images-as-input). In Generative AI Hub on SAP AI Core you can access multiple multimodal models (e.g. `gpt-4o-mini`).\n",
    "\n",
    "üëâ If you have not deployed one of the gpt-4o-mini models in previous exercises, then go back to the model library and deploy a model that can also process images.\n",
    "\n",
    "üëâ Now run the code snippet below to get a description for [the image of the AI Foundation Architecture](documents/ai-foundation-architecture.png). These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "üëâ You can upload your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alternative text:\n",
       "\n",
       "A technical architecture diagram titled \"Single-tenant application (Retrieval Augmented Generation & Generative AI on SAP BTP)\" illustrates the interaction of components on the SAP Business Technology Platform (BTP) for deploying AI solutions. \n",
       "\n",
       "On the left, a \"User\" icon represents users accessing \"Application Clients\" via mobile or desktop devices. These interact with an \"HTML5 App Repository\" and \"App Router\" inside a box labeled \"Subaccount Multi-Cloud\" on SAP BTP. The App Router is connected to \"SAP Authorization and Trust Management service\" denoted by green arrows labeled \"Trust\".\n",
       "\n",
       "Within the Subaccount box, components are divided into:\n",
       "- User Interface: contains \"SAPUI5\" and \"UI5 Web Components\".\n",
       "- SAP Cloud Application Programming Model (CAP): includes \"Application Service\" with items \"Use Case Logic,\" \"Data Management,\" \"LLM Plugins & SDKs,\" and \"SAP Cloud SDK for AI.\"\n",
       "- SAP HANA Cloud: features a \"Vector Engine\" with a grayed-out \"Knowledge Graph Engine.\"\n",
       "- Generative AI Hub: shows multiple layers:\n",
       "  - \"SAP AI Launchpad\"\n",
       "  - \"SAP AI Core\" with \"Trust & Control\" and \"Prompt Registry\"\n",
       "  - \"Orchestration\" containing \"Grounding,\" \"Templating,\" \"Data Masking,\" and \"I/O Filtering\"\n",
       "  - \"Foundation Model Access\": distinguishing \"Partner built\" and grayed-out \"SAP built\"\n",
       "  - \"Foundation Models SAP hosted\"\n",
       "\n",
       "Arrows indicate data flows and interactions: \n",
       "- From Application Clients via Destination to CAP\n",
       "- From CAP to SAP HANA Cloud\n",
       "- From CAP and SAP HANA Cloud to Generative AI Hub\n",
       "- Two-way HTTPS connections from Generative AI Hub to three external network resources on the right:\n",
       "  - SAP On-Premise Solutions via Cloud Connector\n",
       "  - 3rd Party Applications\n",
       "  - SAP Cloud Solutions\n",
       "\n",
       "Legend describes:\n",
       "- Dark blue circles for Access\n",
       "- Green arrows for Mutual Trust\n",
       "- Blue icons for SAP BTP Service\n",
       "- \"xyz\" as planned components.\n",
       "\n",
       "At the bottom, a note states that the diagram is Level L2 and explains it shows how to combine large language models using the Generative AI Hub in SAP AI Core."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# get the image from the documents folder\n",
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\n",
    "            \"role\": \"user\", \n",
    "             \"content\": [\n",
    "                {\"type\": \"text\", \n",
    "                 \"text\": \"Describe the images as an alternative text.\"},\n",
    "                {\"type\": \"image_url\", \n",
    "                 \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from images\n",
    "Nora loves bananabread and thinks recipes are a good example of how LLMs can also extract complex text from images, like from [a picture of a recipe of a bananabread](documents/bananabread.png). Try your own recipe if you like :)\n",
    "\n",
    "This exercise also shows how you can use the output of an LLM in other systems, as you can tell the LLM how to output information, for example in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the extracted ingredients and instructions in two different JSON files:\n",
       "\n",
       "**ingredients.json**\n",
       "```json\n",
       "{\n",
       "  \"dry_ingredients\": {\n",
       "    \"All-purpose Flour\": \"260 g\",\n",
       "    \"Sugar\": \"200 g\",\n",
       "    \"Baking Soda\": \"6 g\",\n",
       "    \"Salt\": \"3 g\"\n",
       "  },\n",
       "  \"wet_ingredients\": {\n",
       "    \"Banana\": \"225 g\",\n",
       "    \"Large Eggs\": \"2\",\n",
       "    \"Vegetable Oil\": \"100 g\",\n",
       "    \"Whole Milk\": \"55 g\",\n",
       "    \"Vanilla Extract\": \"5 g\"\n",
       "  },\n",
       "  \"toppings\": {\n",
       "    \"Chocolate Chips\": \"100 g\",\n",
       "    \"Walnuts (Optional)\": \"100 g\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "**instructions.json**\n",
       "```json\n",
       "{\n",
       "  \"directions\": [\n",
       "    \"Preheat oven to 180 ¬∞C\",\n",
       "    \"Mash banana in a bowl\",\n",
       "    \"Combine banana and other wet ingredients together in the same bowl\",\n",
       "    \"Mix all dry ingredients together in a separate bowl\",\n",
       "    \"Use a whisk to combine dry mixture into wet mixture until smooth\",\n",
       "    \"Pour mixture into a greased/buttered loaf pan\",\n",
       "    \"Place the pan into preheated oven on the middle rack and bake for about 60 minutes or until a toothpick comes out clean\",\n",
       "    \"Enjoy!\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the image from the documents folder\n",
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \n",
    "             \"content\": [\n",
    "                {\"type\": \"text\", \n",
    "                \"text\": \"Extract the ingredients and instructions in two different json files.\"},\n",
    "                {\"type\": \"image_url\", \n",
    "                 \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_data}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=MODEL_NAME, messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next exercise](04-create-embeddings.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
